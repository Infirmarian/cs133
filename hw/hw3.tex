\documentclass[titlepage]{article}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=blue,
}
\urlstyle{same}
\usepackage{xcolor}
\usepackage{listings}
\def\code#1{\texttt{#1}}
\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}
\usepackage{minted}

\title{CS 133 Homework 3}
\author{Robert Geil \\ University of California, Los Angeles}
\begin{document}
\maketitle
%------------------------------------------------------------------------------
\subsubsection*{1. Why do we need the concept of \textit{communicator} in MPI?
What is the default communicator? Assuming that we have 16 processors involved
in the parallel computation, please provide the MPI code to create 4
communicators such that all processors with identical rank mod 4 are in the
same communicator, where rank is the processor ID in the default communicator}
A \textit{communicator} defines a communication domain, which describes the set
of processors which can interconnect with each other. Since each processor 
doesn't have shared memory, there needs to be some explicit grouping of
processors that can send and receive data. The default communicator, 
\code{MPI\_COMM\_WORLD} is a basic communicator that contains all processors.
Custom communicator sets can also be built using the function
\code{MPI\_Comm\_split()}
\begin{minted}[linenos, bgcolor=backgroundColour]{C}
#include <mpi.h>
// Get the rank of each communicator
int world_rank;
MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
// Calculate the new communicator they are part of
int new_comm_index = world_rank % 4;
// Create the new communicator and split
MPI_Comm new_communicator;
MPI_Comm_split(MPI_COMM_WORLD, new_comm_index,\
                world_rank, &new_communicator);
\end{minted}
The above code first gets the rank of each process, then calculates the new
communicator by modding that value by 4. From there, we invoke
\code{MPI\_Comm\_split()}, passing the original communicator
\code{MPI\_COMM\_WORLD} along with our rank, the new communicator index to join
and a reference to our new communicator. This stacks on the existing
communicator such that each processor has both a rank in the default
communicator as well as a rank in our new communicator, both of which can be
accessed through the \code{MPI\_Comm\_rank()} function
%------------------------------------------------------------------------------
\subsubsection*{2. Given a list L of k*N integers of value between 1 to m as
the input evenly distributed among k processors stored in their local file
systems, please write an efficient MPI program to generate the histogram h of
list L at processor 0. Please make your function as efficient as possible, and
highlight the MPI functions that you are using.}
\begin{minipage}{\linewidth}
\begin{minted}[linenos]{C}
#include <mpi.h>
#include <stdlib.h>
#include <string.h>
// L is an array of N integers, each between 1 and m
int* histogram(int* L, int N, int m)
{
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    int* hist = (int*) malloc(sizeof(int) * m);
    memset(hist, 0, sizeof(int)*m);
    // Accumulate into each index of the array m
    // This is done per-processor
    for(int i = 0; i<N; ++i)
    {
        hist[L[i]+1]++;
    }
    // Reduce to the root (0) process, summing each individual
    // histogram and storing in hist
    MPI_Reduce(hist, hist, m, MPI_INT, \
               MPI_SUM, 0, MPI_COMM_WORLD);
    
    // Free memory and return nullptr, except for root where
    // we return a pointer to the histogram
    if(rank){
        free(hist);
        hist = 0;
    }
    return hist;
}
\end{minted}
\end{minipage} \\\\
The above program takes the locally available portions of the list and creates
a histogram of the local portion. From there, we use the MPI function 
\code{MPI\_Reduce} to reduce by summation all of the elements in hist, placing
them in the root process. \code{MPI\_Reduce} distributes the work
across the processors, such that for example we aggregate (0, 1) $->$ 0 and
(2, 3) $->$ 2 in one step, then just need to aggregate 0 and 2, meaning that
we only need $\log(k)$ steps for $k$ processors. This makes it more efficient
than manually passing all the arrays to process 0.
%------------------------------------------------------------------------------
\subsubsection*{3. For the E-cube algorithm for all-to-all personalized 
communication discussed in Lectures 7 and 8, if we implement it on a 
4-dimension hypercube, how many steps will the algorithm goes through? At Step
7, which processor will processor 5 exchange messages?}
The pseudo-code for the \textit{all-to-all personalized} communication is given
below.
\begin{verbatim}
procedure ALL_TO_ALL_PERSONAL(d, my_id)
begin
    for i := 1 to 2^d - 1 do
    begin
        partner := my_id XOR i;
        send M(my_id, partner) to partner;
        receive M(partner, my_id) from partner;
    end for;
end ALL_TO_ALL_PERSONAL
\end{verbatim}
Given that this is taking place on a 4-dimension hypercube, in the example our
value fo $d$ is 4. Therefore our inner loop iteration goes through 
$2^4 - 1 = 15$ times. At step 7, we have $i=7$. We can find the id of the
processor that $x$ is communicating with by taking $\textrm{id}(x) \oplus i$.
In this case, we want $x= 5_{10} = 101_2$ and step $7_{10} = 111_2$ Therefore
we can compute the processor that 5 is communicating with on step 7 will be
$101 \oplus 111 = 010$, which translates to \textbf{processor 4}
%------------------------------------------------------------------------------
\subsubsection*{4. Consider the basic matrix multiplication algorithm for two
NxN matrices A and B using KxK processors connected using a mesh network. 
Assume that each processor already has data the corresponding N/K x N/K 
sub-matrices of A and B, and only need to generate and store the resulting N/K
x N/K sub-matrix locally. Please derive the isoefficiency relation and the
scalability function. You may assume that N is a multiple of K}
The basic sequential time complexity for this matrix multiplication is 
$\Theta(N^3)$. In addition, we are using \textit{Cannon's algorithm}, which
requires that we shift each block to the right neighbor and receive from the
left. This means that we will perform K shiftings over the course of the
multiplication each of size $\frac{N^2}{K^2}$, therefore giving our
communication time of $\Theta(\frac{N^2}{K})$. Our isoefficiency function
therefore gives us
\begin{equation}
    n^3 \geq \frac{Cpn^2}{K}
\end{equation}
which is equivelent to
\begin{equation}
    n \geq CK
\end{equation}
since the number of processors $p$ is equal to $K^2$.
We can then derive the scalability function using
\begin{equation}
    \frac{M(CK)}{K^2} = \frac{C^2K^2}{K^2} = C^2
\end{equation}
Therefore we see that this solution scales very well, as it doesn't increase
communication overhead as more processors are added, since the scalability
function has no reliance on $K$. This makes sense, since as the size of the
mesh of processors increases, the amount of data that must be passed remains
constant, since an increase in size corresponds to a decrease in the individual
N/K blocks that a processor must compute.
%------------------------------------------------------------------------------
\subsubsection*{5. In Lecture 9, we discussed the example shown on the right,
which has a loop pipelining initiation interval equal to 2. If we only want to
output d[SIZE], can you rewrite the code so that the II becomes 1?}
\begin{minted}[linenos]{C}
i = 1;
for (i=1; i<=SIZE; i++) {
    d[i] = d[i-1]*v[i];
}
\end{minted}
With the above code, we see that there is an II of 2, since for each step, we
depend on waiting for the previous step to have stored the old value in 
\code{d[i-1]}. This initiation interval is shown below
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        load v[i] & load d[i-1] & mult & store d[i] & & \\
        \hline 
        & & load v[i] & load d[i-1] & mult & store d[i]\\
        \hline 
    \end{tabular}
\end{center}
If we only care about the the final value, we can elimate the stores to $d$
every iteration, replacing the above code with the following
\begin{minted}[linenos]{C}
    i = 1;
    dsize = d[0];
    for (i=1; i<=SIZE; i++) {
        dsize = dsize * v[i];
    }
\end{minted}
This will not store every round in $d$, meaning that the value of the array
$d$ is not equal to the first version, however, it performs the same operation
on the eventual output, that is multiplying the first element of $d$ with each
element of $v$. This updated version gives us the II = 1, as shown below
\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        load v[i]  & mult & & \\
        \hline 
        & load v[i]  & mult & \\
        \hline 
        & & load v[i]  & mult \\
        \hline
    \end{tabular}
\end{center}
\end{document}