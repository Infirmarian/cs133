
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{pgfplots}
\def\code#1{\texttt{#1}}
% inlined bib file
\usepackage{filecontents}

\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Convolutional Neural Network Acceleration with OpenCL
and FPGA}

%for single author (just remove % characters)
\author{
{\rm Robert Geil}\\
University of California, Los Angeles
} % end author

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract}
%-------------------------------------------------------------------------------
Convolutional Neural Networks or \textbf{CNNs} are used by deep learning to
complete many tasks, and especially image recognition. In this lab, we 
attempted to accelerate the performance of a CNN using OpenCL and Vitis, a tool
provided by Xilinx, as well as Merlin, a similar tool using preprocessor
directives. By applying techniques in the kernel code, such as pipelining and
parallelism, as well as utilizing local memory, we were able to improve
performance on the simulation of the FPGA from the base of about 0.01 GFlops
to around 1.2 to 5 GFlops.
\end{abstract}

%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------

Convolutional Neural Networks are used in a broad range of fields to perform
automatic categorization of images and data, including image classification,
interpretation of medical results and other applications. As such, the process
of running a CNN would benefit greatly from improvements in performance. In
order to drive some of this performance, we turn to a language called
\textit{OpenCL}. OpenCL, created by an industry group including vendors like
AMD, Intel, NVIDIA, Google, and others, is a programming language used for
heterogeneous computing. With OpenCL, one program can be adapted and run on a
CPU, GPU, FPGA or other computing device, affording flexibility and
portability. Using this language, we will improve and parallelize the
performance of our CNN code. OpenCL also relies on a Host-Kernel system, where
setup code to initialize the system is run on a CPU-based host, while the
Kernels can be more specialized machines like GPUs or FPGAs. As such, our focus
is on writing the Kernel code for performance, rather than host code.

%-------------------------------------------------------------------------------
\section{Machine Specifications}
%-------------------------------------------------------------------------------

The machine for which the code was written and hosted on is an Amazon Web
Services (AWS) \textit{c5.4xlarge} virtual machine. This machine, has a 
virtualized Intel Xeon CPU, and is running a custom AMI from AWS which is
intended for development of FPGA boards, although we stopped at the simulation
step and didn't fully synthesize the design to an actual board.

%-------------------------------------------------------------------------------
\section{Solution Approach and Results}
%-------------------------------------------------------------------------------
While both the Vitis and Merlin portions of this lab were solving the same CNN
problem, I approached them differently given the different environments in
which they were being programmed
%-------------------------------------------------------------------------------
\subsection{Vitis}
For my strategy for parallelizing the Vitis code, I started with the provided
code that was given by the TAs.
\subsubsection{Local Memory}
One optimization was moving immediately reused memory from the global memory
space to a local array. This helps improving locality and memory access times.
To do this, I copied both the weight and input arrays into local buffers, as
shown with the weight buffer below
\begin{verbatim}
for(int p = 0; p<kKernel; ++p){
    for(int q = 0; q<kKernel; ++q){
        weight_buf[p][q] = 
            weight[i*kNum*kKernel*kKernel + 
            j*kKernel*kKernel + p*kKernel+q];
    }
}
\end{verbatim}
\subsubsection{Pipelining}
I applied the pragma \code{\_\_attribute\_\_((xcl\_pipeline\_loop))} at several
loops within the program. This allows the code to be executed in a pipeline
manner, letting continuous work occur, accelerating the program.
\subsubsection{Parallelism}
Based on my knowledge of OpenCL with Vitis, there is no way to explicitly
require parallelism. Instead, parallelism is performed automatically when loops
are unrolled. This is a major difference from the Merlin platform, which will
be discussed later on.
\subsubsection{Results}
With the pipelining and local memory applied to the problem, I was able to
reach a final performance of approximately 1.2 GFlops. While this is quite
poor, it still represents a near 10x improvement over the unoptimized code
ported over from previous labs. Due to time constraints and an unfortunate
circumstance requiring me to miss the last discussion, I was not able to really
push performance. Therefore, there was a lot of wasted space and resources on
the FPGA chip, as seen by the utilization of resources
\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        & LUT & FF & DPS & BRAM \\ \hline
        Usage & 40105 & 20856 & 21 & 704 \\ \hline
        Percentage & 3 & ~0 & ~0 & 16 \\ \hline
    \end{tabular}
\end{center}
%-------------------------------------------------------------------------------
\subsection{Merlin}
The Merlin 
%-------------------------------------------------------------------------------
\section{Results}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Challenges}
%-------------------------------------------------------------------------------
One of the biggest challenges I faced through this was working with the
extremely slow synthesis tools provided by Xilinx. Especially with the Merlin
compiler, attempting to paralleize just a 3 deep loop 8 times ended up taking
nearly an hour to build. This inefficiency made it very difficult to quickly
iterate and improve. Another major challenge I faced was that I missed the
discussion on Friday due to a marching band trip, and therefore was rather in
the dark about many of the potential optimizations, making this more
challenging. Finally, in comparison to the CPU and GPU versions, the FPGA
presented an entirely new architecture which I was more unfamiliar with, and 
which therefore was more difficult to optimize for, since typical patterns like
memory tiling and unrolling didn't seem to have the same effect as on a CPU or
GPU.
%-------------------------------------------------------------------------------
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%